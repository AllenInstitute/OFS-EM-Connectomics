{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data into and out of Neuroglancer\n",
    "\n",
    "As part of data analysis, we often need to use neuroglancer to help annotate or provide context for data analysis.\n",
    "\n",
    "Here, we describe the datasets available on [Microns Explorer](https://microns-explorer.org/), explain how to get information out of neuroglancer states, and put the results of some analysis back into neuroglancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from nglui import statebuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data on Microns Explorer\n",
    "\n",
    "Several useful data tables are available Under the Data -> Layer 2/3 page of Microns Explorer, which you can [go directly to here](https://microns-explorer.org/phase1).\n",
    "Due to the nature of connectomics data, the so-called \"Phase 1\" dataset underwent considerable proofreading and our data tables were updated along the way. The data release \"v185\" refers to the final state of proofreading before moving on to other tasks.\n",
    "At the v185 stage, all neurons with cell bodies in the volume had been extensively proofread, as well as Chandelier cell axons.\n",
    "\n",
    "In our work, we generally use [Pandas](https://pandas.pydata.org/) dataframes for organizing our data.\n",
    "If you aren't familiar with Pandas, there is [extensive documentation here](https://pandas.pydata.org/docs/user_guide/index.html).\n",
    "In short, dataframes are a data representation where each row is a data point and each column is one property of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soma/Cell Type Table\n",
    "\n",
    "It is important to know what cells you are working with.\n",
    "The soma table has basic information about the location and basic cell type (Excitory, Inhibitory, or Glia) of all cells with soma in the volume.\n",
    "This can be used to look up specific neurons by cell type or provide relative location of positions in the dataset to a cell body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_table_file = 'data/pni_synapses_v185.csv'\n",
    "soma_df = pd.read_csv(soma_table_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row describes a different cell.\n",
    "Let's look at each column:\n",
    "\n",
    "**Row and cell type**\n",
    "* `id`: Unique annotation id for the synapse\n",
    "* `cell_type`: Valence of each cell: `e` for excitatory, `i` for inhibitory, `g` for glia.\n",
    "* `pt_root_id`: Unique id for the cell.\n",
    "\n",
    "**Location in nanometer coordinates**\n",
    "* `soma_x_nm`, `soma_y_nm`, `soma_z_nm`: The x/y/z location of the center of the cell body in nanometer coordinates. Note that neuroglancer uses voxels, not nanometers.\n",
    "\n",
    "**Location in voxel coordinates**\n",
    "* `pt_position`: A 3-element location of the presynaptic site in voxel coordinates. Note that loaded this way, this is imported as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For convenience, let's turn the `pt_position` string into a numeric list with some simple python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pt_position strings to a list:\n",
    "\n",
    "def position_string_to_array(pos_string):\n",
    "    vals = pos_string[1:-1].split(' ')\n",
    "    return [int(x) for x in vals if len(x) > 0]\n",
    "\n",
    "# \"apply\" will apply this function to every element of the column\n",
    "soma_df['pt_position'] = soma_df['pt_position'].apply(position_string_to_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### How do these values relate to what you see in Neuroglancer?\n",
    "\n",
    "Let's look at a random row from the data perspective and in neuroglancer.\n",
    "To open a new Neuroglancer window, go to http://layer23.microns-explorer.org.\n",
    "\n",
    "First, copy the coordinate in pt_position and paste it into the location field in the upper left of the neuroglancer window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_df.loc[[10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will put you right in the middle of the nucleus.\n",
    "If you double click it, you'll find that, as expected from the `cell_type` being `e`, this is a pyramidal cell as evident from the dendritic spines.\n",
    "Moreover, if you look at the object id on the \"rendering\" tab on the right, this will match the value in `pt_root_id`.\n",
    "\n",
    "Consistent root ids across data tables and neuroglancer let us keep track of cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synapse Table\n",
    "\n",
    "The biggest data table is the synapse table (370 MB), which has a row for each of the 3.2 million synapses in the data set.\n",
    "Here, we explain what is in this data file.\n",
    "\n",
    "\n",
    "Here, each row is a synapse detected by the machine learning method described in [Turner et al. 2019](https://arxiv.org/abs/1904.09947).\n",
    "The data in this table has approximately 90% precision and 90% recall, but has not been explicitly proofread.\n",
    "As with any output of a machine learning pipeline, this means there are some false positives and false negatives in the data.\n",
    "If you run across something very surprising, it's always valuable to double check the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synapse table into pandas\n",
    "synapse_table_file = 'data/pni_synapses_v185.csv'\n",
    "synapse_df = pd.read_csv(synapse_table_file)\n",
    "synapse_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row describes the a different aspect of the synapse.\n",
    "Let's look at each column:\n",
    "\n",
    "**Annotation and connectivity**\n",
    "* `id`: Unique annotation id for the synapse\n",
    "* `pre_root_id`: Object id of the presynaptic neuron\n",
    "* `post_root_id`: Object id of the postsynaptic neuron\n",
    "\n",
    "**Synapse size**\n",
    "* `cleft_vx`: Size of the synaptic cleft in total voxel count. Generally proportional to surface area.\n",
    "\n",
    "**Location in nanometer coordinates**\n",
    "* `ctr_pt_x_nm`, `ctr_pt_y_nm`, `ctr_pt_z_nm`: The x/y/z location of the synaptic cleft in nanometer coordinates. Note that neuroglancer uses voxels, not nanometers.\n",
    "\n",
    "**Location in voxel coordinates**\n",
    "* `pre_pt_x_vx`, `pre_pt_y_vx`, `pre_pt_z_vx`: The x/y/z location of the presynaptic site in voxel coordinates.\n",
    "* `ctr_pt_x_vx`, `ctr_pt_y_vx`, `ctr_pt_z_vx`: The x/y/z location of the synaptic cleft in voxel coordinates.\n",
    "* `post_pt_x_vx`, `post_pt_y_vx`, `post_pt_z_vx`: The x/y/z location of the postsynaptic site in voxel coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### How do these values relate to what you see in Neuroglancer?\n",
    "\n",
    "Go to http://layer23.microns-explorer.org.\n",
    "\n",
    "Let's bring up one synaptic input onto the cell we looked up before.\n",
    "We do this by selecting all synapses whose `post_root_id` is the root id in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell id copied from above\n",
    "root_id = 648518346349539095\n",
    "\n",
    "# Pandas has a handy `query` function that makes selecting data particularly easy\n",
    "synapse_df.query('post_root_id == @root_id' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to the location for one of the synapses.\n",
    "For simplicity's sake, let's focus on one random row.\n",
    "First, let's look at the center points.\n",
    "Copy the values from `ctr_pos_x_vx` to `ctr_pos_z_vx` for one of the rows and paste it into neuroglancer.\n",
    "If you click the segmentation on the other side of the synapse, it has an id that matches the `pre_root_id` value.\n",
    "Try again with the `pre_pos_x/y/z_vx` and `post_pos_x/y/z_vx` and you'll see that they go to points within the segmentation on either side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_df.query('post_root_id == @root_id' ).iloc[[1320]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proofread Soma Subgraph Synapses\n",
    "\n",
    "This is a subset of the synapse table that is only synapses between excitatory neurons with cells in the volume.\n",
    "This has only 1962 synapses, but all of them have been manually inspected and have a spine head volume associated with them.\n",
    "From a data standpoint, it is similar to the synapse table and thus we will not go into detail here in the tutorial, but we encourage you to look at it if those properties sound useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making your own annotations in neuroglancer\n",
    "\n",
    "You can use \"export csv\" button in the annotation tab to save manual annotations to your computer.\n",
    "By default, annotations just have points associated with them.\n",
    "If you want to also get the root id of the objects that you click on, after you create an annotation layer go to the top of its panel and set \"Linked Segmentation\" to the name of the segmentation layer.\n",
    "\n",
    "Note that not every annotation uses every column.\n",
    "Some annotations like lines or bounding boxes have multiple points associated with them, and an ellipsoid annotation has a three radius dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_filename = 'data/annotations_spines.csv'\n",
    "manual_df = pd.read_csv(annotation_filename)\n",
    "\n",
    "manual_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Manipulating data and viewing it in Neuroglancer\n",
    "\n",
    "Visualizing data in python using packages like matplotlib, seaborn, and plotly is fantastic and well-documented.\n",
    "However, one useful thing is to visualize spatial data in neuroglancer itself, which lets us quickly look at the context of data points in terms of imagery, morphology, and all of the rest of the data we can explore there.\n",
    "\n",
    "Let's start by digging into inhibitory neurons a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's use the cell type table to get the collection of inhibitory cells.\n",
    "soma_df.query('cell_type == \"i\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using StateBuilder to make custom Neuroglancer links\n",
    "\n",
    "While we could jump to pulling those up in neuroglancer one by one, let's do something a little fancier.\n",
    "We will make a neuroglancer link that will have the cell bodies of all inhibitory neurons.\n",
    "\n",
    "StateBuilder in the [nglui package](https://github.com/seung-lab/NeuroglancerAnnotationUI) is designed to turn dataframes into annotations and selected neurons in Neuroglancer.\n",
    "\n",
    "You were introduced to Neuroglancer through each layer.\n",
    "StateBuilder works the same way, configuring each layer and mapping rules from dataframe columns to Neuroglancer properties.\n",
    "We start by configuring the image layer and the segmentation layer.\n",
    "For the most basic situations like this, all we have to know is the cloud-path to the data sources that neuroglancer uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nglui import statebuilder\n",
    "\n",
    "image_path = 'precomputed://gs://microns_public_datasets/pinky100_v0/son_of_alignment_v15_rechunked'\n",
    "segmentation_path = 'precomputed://gs://microns_public_datasets/pinky100_v185/seg'\n",
    "\n",
    "img = statebuilder.ImageLayerConfig(image_path)\n",
    "seg = statebuilder.SegmentationLayerConfig(segmentation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a simple Statebuilder object out of a list of these layer configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = statebuilder.StateBuilder([img, seg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StateBuilder separates the configuration rules from the rendering.\n",
    "In this case, we haven't added any data-driven options to the configuration, so we can render out the state immediately.\n",
    "The `render_as` argument lets us choose to return an HTML link, a long URL, or a dict describing the state.\n",
    "All are useful in different situations, but for manual exploration `html` is cleanest.\n",
    "\n",
    "The link below should bring you into the center of the data with nothing selected. Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.render_state(return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the soma location and root id of each inhibitory neuron.\n",
    "We have already defined the image and segmentation layers, so we only need to add an annotation layer.\n",
    "However, we can't just add an annotation layer, we also need to specify how to map data to annotations.\n",
    "In StateBuilder, this is handled through different Mapper classes: `PointMapper`, `LineMapper`, `SphereMapper`, and `BoundingBoxMapper`.\n",
    "We will use a `PointMapper` so that each soma location gets one point.\n",
    "\n",
    "We are going to eventually pass a dataframe to the statebuilder.\n",
    "A `PointMapper` object needs to know which column of the dataframe contains points.\n",
    "Note that Neuroglancer expects locations in voxel coordinates, not nanometers.\n",
    "\n",
    "It would also be convenient to put the root id of each cell in as well.\n",
    "For the PointMapper, this involves setting the `linked_segmentation_column`.\n",
    "However, the annotation layer also has to know where to look up root ids from, so you have to specify the name of the segmentation layer as well in the `AnnotationLayerConfig`.\n",
    "\n",
    "Now we need to add an annotation layer using an `AnnotationLayerConfig` and set the mapping rules to be the PointMapper.\n",
    "We can then make a new statebuilder with this third layer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = statebuilder.PointMapper('pt_position', linked_segmentation_column='pt_root_id')\n",
    "anno_layer = statebuilder.AnnotationLayerConfig('soma_points', mapping_rules=points, linked_segmentation_layer=seg.name)\n",
    "sb = statebuilder.StateBuilder([img, seg, anno_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we pass the dataframe to the StateBuilder on render to show the location of all inhibitory cells.\n",
    "\n",
    "One handy feature of neuroglancer is using the square brackets `[`/`]` to move between annotations in a list.\n",
    "If a linked segmentation is set, as it is here, Neuroglancer will also download the mesh so you can quickly visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.render_state(soma_df.query('cell_type==\"i\"'), return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number and properties of the annotations are drawn from the dataframe.\n",
    "The exact same configuration can be used to show excitatory cells just by changing the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.render_state(soma_df.query('cell_type==\"e\"'), return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling it together to do some analysis\n",
    "\n",
    "* Use groupby to get a list of output synapses per inhibitory cell.\n",
    "* Use merge to bring soma position into the same dataframe as cells and compute Euclidean distance from soma.\n",
    "* Use groupby to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inhib_syn_df = synapse_df.query('pre_root_id in @inhibitory_root_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_count_df = inhib_syn_df[['pre_root_id', 'cleft_vx']].groupby('pre_root_id').count()\n",
    "syn_count_df = syn_count_df.rename(columns={'cleft_vx': 'num_pre'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_count_df.query('num_pre > 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_output_inhibitory_root_ids = syn_count_df.query('num_pre > 100').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inhib_syn_soma_df = inhib_syn_df.merge(soma_df[['pt_root_id', 'pt_position']], left_on='post_root_id', right_on='pt_root_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synapse_dist_from_soma(row):\n",
    "    syn_loc = np.array([row['ctr_pt_x_nm'], row['ctr_pt_y_nm'], row['ctr_pt_z_nm']])\n",
    "    soma_loc = np.array(row['pt_position']) * [4,4,40]\n",
    "    return np.linalg.norm(syn_loc-soma_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inhib_syn_soma_df['dist_from_soma'] = inhib_syn_soma_df.apply(synapse_dist_from_soma, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inhib_syn_soma_df['is_soma_synapse'] = inhib_syn_soma_df['dist_from_soma'] < 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inhib_syn_soma_count = inhib_syn_soma_df.groupby(['pre_root_id', 'is_soma_synapse']).count().loc[high_output_inhibitory_root_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inhib_syn_soma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_id = 648518346349539215\n",
    "inhib_syn_soma_df.query('pre_root_id == @root_id and is_soma_synapse==True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inhib_syn_soma_df['ctr_position'] = inhib_syn_soma_df.apply(lambda x: [x['ctr_pos_x_vx'], x['ctr_pos_y_vx'], x['ctr_pos_z_vx']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = statebuilder.ImageLayerConfig(image_path)\n",
    "seg = statebuilder.SegmentationLayerConfig(segmentation_path, selected_ids_column='pre_root_id')\n",
    "\n",
    "points = statebuilder.PointMapper('ctr_position', linked_segmentation_column='post_root_id')\n",
    "anno_layer = statebuilder.AnnotationLayerConfig('soma_synapses', mapping_rules=points, linked_segmentation_layer=seg.name)\n",
    "sb = statebuilder.StateBuilder([img, seg, anno_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_id = 648518346349539215\n",
    "sb.render_state(inhib_syn_soma_df.query('pre_root_id == @root_id and is_soma_synapse == True'), return_as='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_id = 648518346349539215\n",
    "sb.render_state(inhib_syn_soma_df.query('pre_root_id == @root_id and is_soma_synapse == False'), return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dash to integrate exploratory analysis and statebuilder into interactive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_root_ids = soma_df['pt_root_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_synapse_df = synapse_df.query('pre_root_id in @soma_root_ids or post_root_id in @soma_root_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dashdataframe import configure_app\n",
    "# initialize a dash app\n",
    "app = dash.Dash()\n",
    "\n",
    "# configure the app\n",
    "configure_app(app, df)\n",
    "\n",
    "# run the dash server\n",
    "app.run_server(port=8880)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
